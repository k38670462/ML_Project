# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O-lJmQyG109OB4V89pDUYMtLNuHo2vZG
"""

# -*- coding: utf-8 -*-


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import csv
import math
import random


training_datalist =  []
testing_datalist =  []
output_datalist =  []

training_dataset = []
validation_dataset = []
y = []
y_val = []
coefficient_output = []



training_dataroot = 'data-1-year.csv'
# testing_dataroot = 'hw1_advanced_testing.csv'
# output_dataroot = 'hw1_advanced.csv'
coefficient_output_dataroot = 'model_coefficient.csv'


W = []
B = []


with open(training_dataroot, newline='') as csvfile:
  training_datalist = np.array(list(csv.reader(csvfile)))
  #print(training_datalist)

#with open(testing_dataroot, newline='') as csvfile:
#  testing_datalist = np.array(list(csv.reader(csvfile)))
#  #print(testing_datalist)



def SplitData():
    global training_dataset, y, training_datalist
    #print(len(training_datalist[0]))
    y = np.array([[float(training_datalist[i][-1])] for i in range(1, len(training_datalist))])
    for i in range(1, len(training_datalist)):
        p = []
        for j in range(len(training_datalist[i])-1):
            p.append(float(training_datalist[i][j].replace(',','')))
        training_dataset.append(p)
    training_dataset = np.array(training_dataset)

def PreprocessData():
  global training_dataset, y
  index = [1197]
  training_dataset = np.delete(training_dataset, index, axis = 0)
  y = np.delete(y, index, axis = 0)


def train_val_split(X, y, val_size, random_state):
    np.random.seed(random_state)
    np.random.shuffle(X)
    np.random.shuffle(y)

    X_train = X[: int(len(X) - val_size)]
    X_val = X[-int(val_size) :]

    y_train = y[: int(len(y) - val_size)]
    y_val = y[-int(val_size) :]
    return X_train, X_val, y_train, y_val

def GradientDescent():
    global W, B, coefficient_output, training_dataset, y
    W = np.random.randn(len(training_dataset[0]))
    B = np.random.randn()
    iter_num = 151
    rate = 0.0000001
    pred = []
    for _ in range(1, iter_num):
        pred = np.dot(training_dataset, W) + B
        loss = np.mean((pred - y) ** 2)
        gradient_w = np.dot(training_dataset.T, (pred - y)) / len(y)
        gradient_b = np.mean(pred - y)
        W -= rate * np.mean(gradient_w, axis=1)
        B -= rate * gradient_b
        #if _ % 10 == 0:
        coefficient_output.append(np.concatenate((W, np.array([B]))))
        print(f'{_}: ', sum(map(lambda x, z: abs((z-x)/z), pred, y))/len(y)*100)

def MakePrediction():
    global validation_dataset, W, B, y_val
    pred = np.dot(validation_dataset, W) + B
    print(f'pred: ', sum(map(lambda x, z: abs((z-x)/z), pred, y_val))/len(y_val)*100)
    #print(output_datalist)

SplitData()
#print(len(y))
PreprocessData()
#print(training_dataset)
#print(len(y))
training_dataset, validation_dataset, y, y_val = train_val_split(training_dataset, y, 0.1 * len(y), 22)
#print(len(training_dataset))
#print(len(training_dataset[0]))
GradientDescent()
MakePrediction()

'''
output_datalist = list(map(lambda x: [x], output_datalist))
with open(output_dataroot, 'w', newline='', encoding="utf-8") as csvfile:
  writer = csv.writer(csvfile)
  for row in output_datalist:
    writer.writerow(row)
'''

with open(coefficient_output_dataroot, 'w', newline='', encoding="utf-8") as csvfile:
  writer = csv.writer(csvfile)
  for row in coefficient_output:
    writer.writerow(row)